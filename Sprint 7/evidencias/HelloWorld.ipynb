{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49323)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('SparkHelloWorld') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|\n",
      "+---------+----------+--------+-----+---+------+\n",
      "|    James|          |   Smith|36636|  M|  3000|\n",
      "|  Michael|      Rose|        |40288|  M|    -1|\n",
      "|   Robert|          |Williams|42114|  M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|\n",
      "|      Jen|      Mary|   Brown| NULL|  F|  3000|\n",
      "+---------+----------+--------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "    (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", -1),\n",
    "    (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000),\n",
    "    (\"Maria\", \"Anne\", \"Jones\", \"39192\", \"F\", 4000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", None, \"F\", 3000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True), \\\n",
    "    StructField(\"middlename\", StringType(), True), \\\n",
    "    StructField(\"lastname\", StringType(), True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"sex\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar (similar a SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|\n",
      "+---------+----------+--------+-----+---+------+\n",
      "|    James|          |   Smith|36636|  M|  3000|\n",
      "|   Robert|          |Williams|42114|  M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|\n",
      "+---------+----------+--------+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_filtered = df.where(\n",
    "    (col(\"id\").isNotNull()) & (col(\"salary\") > 0)\n",
    "    \n",
    ")\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenar os nomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|      fullname|\n",
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|    JamesSmith|\n",
      "|   Robert|          |Williams|42114|  M|  4000|RobertWilliams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|MariaAnneJones|\n",
      "+---------+----------+--------+-----+---+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "df_full_name = df_filtered.withColumn(\n",
    "    \"fullname\",\n",
    "    concat(col(\"firstname\"), col(\"middlename\"), col(\"lastname\"))\n",
    ")\n",
    "\n",
    "df_full_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como separar os nomes com espaços?\n",
    "É necessário utilizar a função lit(), a qual faz a conversão de um tipo primário para uma versão no formato coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|        fullname|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|    James  Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000|Robert  Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Anne Jones|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_full_name2 = df_filtered.withColumn(\n",
    "    \"fullname\",\n",
    "    concat(\n",
    "        col(\"firstname\"),\n",
    "        lit(\" \"),\n",
    "        col(\"middlename\"),\n",
    "        lit(\" \"),\n",
    "        col(\"lastname\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_full_name2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação se a coluna middlename está vazia (ficou 2 espaços em James Smith)\n",
    "Para isso utilizamos CASE WHEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|        fullname|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|     James Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000| Robert Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Anne Jones|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df_full_name3 = spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "    CASE\n",
    "        WHEN middlename = '' THEN concat(firstname, \" \", lastname) \n",
    "        ELSE concat(firstname, \" \", middlename, \" \", lastname)\n",
    "    END AS fullname\n",
    "    FROM df                     \n",
    "    \"\"\")\n",
    "\n",
    "df_full_name3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código acima, criamos uma TempView, assim como no SQL, chamada df para podermos acessar via query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código usando DataFrame APIs com mesmo resultado da consulta acima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|firstname|middlename|lastname|   id|sex|salary|        fullname|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "|    James|          |   Smith|36636|  M|  3000|     James Smith|\n",
      "|   Robert|          |Williams|42114|  M|  4000| Robert Williams|\n",
      "|    Maria|      Anne|   Jones|39192|  F|  4000|Maria Anne Jones|\n",
      "+---------+----------+--------+-----+---+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_full_name4 = df_filtered.withColumn(\n",
    "    \"fullname\",\n",
    "    when(\n",
    "        col(\"middlename\") == \"\",\n",
    "        concat(\n",
    "            col(\"firstname\"),\n",
    "            lit(\" \"),\n",
    "            col(\"lastname\")\n",
    "        )\n",
    "    ).otherwise(\n",
    "        concat(\n",
    "        col(\"firstname\"),\n",
    "        lit(\" \"),\n",
    "        col(\"middlename\"),\n",
    "        lit(\" \"),\n",
    "        col(\"lastname\")\n",
    "    )\n",
    "    )\n",
    ")\n",
    "\n",
    "df_full_name4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão: Escrever código utilizando as APIs da classe DataFrame torna mais fácil debugar o código em caso de erro e dividir o código em funções menores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
