# üß© Desafio da Sprint 9
Este diret√≥rio cont√©m os arquivos necess√°rios para a realiza√ß√£o do desafio desta Sprint.

___

## üìù Enunciado
O desafio da Sprint 9 √© uma continua√ß√£o do desafio iniciado na Sprint 6, sendo a quarta entrega do desafio final.

Esta etapa consiste na **modelagem dos dados e processamento da Camada *refined***. Nessa camada, os dados devem estar prontos para an√°lise e extra√ß√£o de insights. A origem correspondente desses dados deve ser a camada Trusted, processada na Sprint 8.

Nesta Sprint devemos pensar em estruturar os dados seguindo os princ√≠pios de modelagem multidimensional, a fim de permitir consultas sobre diferentes perspectivas.

Ser√° novamente utilizado o Apache Spark no processo, utilizando jobs cuja origem sejam dados da Trusted Zone, e o destino a camada Refined Zone. Os dados devem ser novamente persistidos no formato PARQUET, particionados, se necess√°rio, de acordo com as necessidades definidas para a camada de visualiza√ß√£o.

### Afazeres
- Criar tabelas no AWS Glue Data Catalog.
- Se necess√°rio, criar *views* de acordo com a modelagem de dados solicitada.
- Criar camada *Refined*, tendo como origem os dados da camada *Trusted*.

### Entreg√°veis
- Arquivo markdown (este README) com evid√™ncias da realiza√ß√£o do desafio + documenta√ß√£o de cada parte executada.
- Modelo de dados da camada Refined desenhado em ferramenta de modelagem.
- C√≥digo desenvolvido com devidos coment√°rios.

--- 

## Resolu√ß√£o

### 1. Quest√µes para an√°lise

As quest√µes para an√°lise n√£o foram alteradas, seguem as mesmas definidas na Sprint 7:

1. Quais foram os principais marcos que consolidaram Christopher Nolan como um dos diretores mais influentes do cinema mundial? 

2. M√©dia de bilheteria de todos os filmes do ano 2000 e comparar com Memento (Amn√©sia) - primeiro filme de sucesso de Nolan.

3. Avalia√ß√£o m√©dia de filmes de her√≥is em compara√ß√£o √† m√©dia dos filmes do Batman de Nolan.

4. Para colocar suas ideias em pr√°tica, Nolan precisa de or√ßamentos maiores do que outros filmes similares?


### 2. Modelagem dos dados

Por conseguinte, realizei a modelagem dos dados. Necessitei rever conceitos aprendidos e aplicados na Sprint 2 (modelagem dimensional - *star schema* e *snowflake*) para realizar essa tarefa.

Para realizar a modelagem, utilizei a ferramenta **brmodelo**, criando os seguintes fatos e dimens√µes:

![Imagem modelagem dos dados](../evidencias/2-modelagem.png)

- **dim_tempo:** cont√©m a data de lan√ßamento do filme, al√©m dos campos ano, m√™s e dia, para facilitar an√°lises posteriores.
- **dim_diretor:** cont√©m o nome do diretor, no caso apenas filmes de Christopher Nolan conter√£o, os outros n√£o foram coletados, pois n√£o havia necessidade.
- **dim_genero:** cont√©m o(s) g√™nero(s) dos filmes.
- **dim_titulo:** cont√©m o t√≠tulo dos filmes.
- **fato_filme:** considerei o filme como o fato, contendo todas as informa√ß√µes num√©ricas do meu dataset: id, n√∫mero de votos, nota m√©dia, or√ßamento e receita.

As cardinalidades s√£o todas **1-n** devido √† limita√ß√£o da ferramenta brmodelo, por√©m entre o fato e a dimens√£o tempo deveria ser **1-1**, e a cardinalidade do lado da dimens√£o g√™nero deveria ser **1-n**, n√£o 0-n. O restante est√° correto.


### 3. Cria√ß√£o do script local

Primeiramente optei por criar o script *on-premise* para posteriormente levar ao AWS Glue e executar em um job.

**3.1 - Download dos arquivos .parquet**

Busquei os arquivos .PARQUET processados na Sprint anterior e localizados na camada Trusted Zone para t√™-los em minha m√°quina.

![Imagem download parquet](../evidencias/3.1-download-parquet.png)

**3.2 - Cria√ß√£o do script**

- **3.2.1**
    - Primeiro, criei um script que lia os arquivos .parquet e exibia os schemas, para entender todos os campos e como faria organizaria os dados para a modelagem efetuada.

![Imagem printSchemas](../evidencias/3.2-schemas.png)

- **3.2.2**
    - Posteriormente, realizei alguns testes de jun√ß√£o entre os dados do CSV (batch) e do TMDB, unindo pelo t√≠tulo do filme, haja vista que os ids eram diferentes.
    - Al√©m disso, removi campos desnecess√°rios, que n√£o seriam efetivamente utilizados na an√°lise final.

![Imagem dataframes unidos e limpos](../evidencias/3.3-dfLimpo.png) 








### 4. Job AWS Glue

**4.1 - Cria√ß√£o do job**

Primeiramente, criei o job no AWS Glue conforme solicitado no enunciado do exerc√≠cio. A imagem abaixo comprova a cria√ß√£o do job.

![Imagem cria√ß√£o do job no AWS Glue](../evidencias/4.1-criacao-job.png)

**4.2 - Ajuste de par√¢metros do job**

Agora, inseri os par√¢metros com local de origem dos dados (camada Trusted) e onde eles ser√£o salvos ap√≥s execu√ß√£o do script (camada Refined).

![Imagem par√¢metros job](../evidencias/4.2-parametros-job.png)

**4.3 - Cria√ß√£o do script do job**

Com o job criado e os par√¢metros definidos, iniciei a constru√ß√£o do [script](script-glue.py) do job no Glue. Para isso, utilizei como base o script local desenvolvido anteriormente.

1. Imports necess√°rios

![Imagem imports](../evidencias/4.3-script1.png)

- O c√≥digo importa bibliotecas essenciais para executar um job no AWS Glue e manipular dados com PySpark.
- Bibliotecas essenciais para o uso do AWS Glue, o qual √© utilizado para ETL (*Extract*, *Transform*, *Load*) na AWS.
- PySpark: usado para manipula√ß√£o e processamento de dados distribu√≠dos.
- Fun√ß√£o `col` do PySpark para tratamento das colunas.


2. Configura√ß√µes de ambiente (par√¢metros, SparkContext e caminhos de arquivos)

![Imagem configura√ß√µes de ambiente](../evidencias/4.3-script2.png)

- **Linha 11:** `getResolvedOptions` obt√©m os par√¢metros necess√°rios para rodar o job. Eles incluem o nome do job, o caminho de entrada `(S3_INPUT_PATH)`, e o caminho de sa√≠da `(S3_TARGET_PATH)`.
- **Linhas 14-18:** configura√ß√µes necess√°rias para script no AWS Glue (fornecido pelo pr√≥prio servi√ßo ao criar o job).
- **Linhas 20-21:** defini√ß√£o dos caminhos dos arquivos de entrada e sa√≠da, Trusted e Refined Zone respectivamente.
- **Linhas 23-24:** Leitura dos arquivos PARQUET para Spark DataFrames: `df_csv` com dados da ingest√£o batch (local) e `df_tmdb` com os dados da ingest√£o da API do TMDB.


3. Transforma√ß√µes nos DataFrames

![Imagem transforma√ß√µes DataFrames](../evidencias/4.3-script3.png)

- **Linha 27:** removi o id do `df_csv`, pois ambos os DataFrames possu√≠am id, por√©m esse id n√£o era o mesmo, para que pudesse ser realizado um `join` por esse id. Por isso, escolhi remover o DataFrame do CSV.
- **Linha 28:** transformei a coluna `id` do DataFrame do TMDB em `long`, haja vista que ela era do tipo String.
- **Linha 31:** uni os DataFrames baseando-me pelo t√≠tulo do filme. Apesar de n√£o ser a estrat√©gia mais adequada, n√£o havia outra informa√ß√£o (esperava que fosse o id) para realizar o `join`.
- **Linha 34:** removi alguns campos que julguei n√£o serem mais necess√°rios para a minha an√°lise final.
- **Linha 35:** removi os duplicados pelo id do filme.
- **Linha 38:** printei o Schema do DataFrame para entender como havia ficado ap√≥s essas transforma√ß√µes.


4. Ajuste no local de salvamento dos arquivos PARQUET

![Imagem salvamento PARQUET Movies](../evidencias/4.3-script4.png)

- Necessitei ajustar o local de salvamento dos arquivos PARQUET, inserindo mais um diret√≥rio: "Movies".

5. Tabela Fato `fato_filme`

![Imagem tabela fato_filme](../evidencias/4.3-script5.png)

- Para criar a tabela fato_filme, que conteria todos os campos num√©ricos e que se relacionaria com as demais dimens√µes, selecionei os campos: "id", "numeroVotos", "notaMedia", "orcamento" e "receita".
- Os dados s√£o salvos no formato parquet no diret√≥rio /fato_filme dentro do caminho de destino.

6. Tabela Dimensional `dim_titulo`

![Imagem tabela dim_titulo](../evidencias/4.3-script6.png)

- A tabela armazena o id e o titulo do filme, e os dados s√£o salvos no diret√≥rio /dim_titulo.

7. Tabela Dimensional `dim_tempo`

![Imagem tabela dim_tempo](../evidencias/4.3-script7.png)

- Colunas ano, mes, e dia s√£o extra√≠das da coluna data_lancamento.
- Os dados s√£o gravados no diret√≥rio /dim_tempo.

8. Tabela Dimensional dim_genero

![Imagem tabela dim_genero](../evidencias/4.3-script8.png)

- Seleciona as colunas id e genero e grava os dados no diret√≥rio /dim_genero.


9. Tabela Dimensional dim_diretor

![Imagem tabela dim_diretor](../evidencias/4.3-script9.png)

- Verifica se a coluna produtoras existe e cont√©m valores n√£o nulos. Se sim, cria-se uma coluna diretor com o valor fixo "Christopher Nolan" e grava esses dados no diret√≥rio /dim_diretor.

> Fiz essa valida√ß√£o, pois ao extrair os dados da API do TMDB, apenas os filmes cujas produtoras foram resgatadas pertencem √† Christopher Nolan. Isso ocorreu, pois n√£o busquei os diretores de todos os filmes, portanto a distin√ß√£o entre ser de Christopher Nolan, ou n√£o, ficou a crit√©rio de possuir ou n√£o produtoras.

10. Finaliza√ß√£o do job

![Imagem job commit](../evidencias/4.3-script91.png)

**4.4 - Job Runs**

Ap√≥s desenvolver o script, executei o job. Fiz isso por diversas vezes, pois obtive erros no meio do caminho, os quais tiveram de ser solucionados resultando no script explicitado no item anterior.

![Imagem job runs](../evidencias/4.4-jobRuns.png)

**4.5 - Diret√≥rios criados**

Os diret√≥rios criados ap√≥s a execu√ß√£o do job ficaram da seguinte maneira:

![Imagem diret√≥rios p√≥s execu√ß√£o do job](../evidencias/4.5-diretoriosExecucao.png)


### 5. Crawler

**5.1 - Cria√ß√£o**

Para criar as tabelas conforme requisitado, necessitei criar novamente um *crawler*, da mesma forma que na Sprint anterior. A imagem a seguir comprova a cria√ß√£o do Crawler, com a origem dos dados sendo a Refined Zone.

![Imagem cria√ß√£o Crawler](../evidencias/5.1-criandoCrawler.png)

**5.2 - Execu√ß√£o**

Ap√≥s criar o *crawler*, executei-o para que ele criasse as tabelas baseado nos arquivos PARQUET gerados.

![Imagem execu√ß√£o crawler](../evidencias/5.2runCrawler.png)

**5.3 - Consulta das tabelas criadas**

Com isso, verifiquei se as tabelas haviam sido criadas corretamente no AWS Glue Data Catalog.

![Imagem tabelas criadas](../evidencias/5.3-tabelasCriadas.png)

**5.4 - Consulta no Amazon Athena**

Por fim, performei uma consulta no Amazon Athena para ver se as tabelas estavam preenchidas corretamente conforme o script desenvolvido.

![Imagem consulta Athena](../evidencias/5.4-consultaAthena.png)

___

### ‚Ü©Ô∏è [Retornar ao in√≠cio](../../README.md)