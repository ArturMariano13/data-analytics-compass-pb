# üß© Desafio da Sprint 9
Este diret√≥rio cont√©m os arquivos necess√°rios para a realiza√ß√£o do desafio desta Sprint.

___

## üìù Enunciado
O desafio da Sprint 9 √© uma continua√ß√£o do desafio iniciado na Sprint 6, sendo a quarta entrega do desafio final.

Esta etapa consiste na **modelagem dos dados e processamento da Camada *refined***. Nessa camada, os dados devem estar prontos para an√°lise e extra√ß√£o de insights. A origem correspondente desses dados deve ser a camada Trusted, processada na Sprint 8.

Nesta Sprint devemos pensar em estruturar os dados seguindo os princ√≠pios de modelagem multidimensional, a fim de permitir consultas sobre diferentes perspectivas.

Ser√° novamente utilizado o Apache Spark no processo, utilizando jobs cuja origem sejam dados da Trusted Zone, e o destino a camada Refined Zone. Os dados devem ser novamente persistidos no formato PARQUET, particionados, se necess√°rio, de acordo com as necessidades definidas para a camada de visualiza√ß√£o.

### Afazeres
- Criar tabelas no AWS Glue Data Catalog.
- Se necess√°rio, criar *views* de acordo com a modelagem de dados solicitada.
- Criar camada *Refined*, tendo como origem os dados da camada *Trusted*.

### Entreg√°veis
- Arquivo markdown (este README) com evid√™ncias da realiza√ß√£o do desafio + documenta√ß√£o de cada parte executada.
- Modelo de dados da camada Refined desenhado em ferramenta de modelagem.
- C√≥digo desenvolvido com devidos coment√°rios.

--- 

## Resolu√ß√£o

### PROBLEMA IDENTIFICADO NOS DADOS AO REALIZAR CONSULTAS

**1. PROBLEMAS**

Ao realizar consultas no Amazon Athena, percebi problemas nos dados. 

![Imagem problema nos dados](../evidencias/0.0-erroDados.png)

**Problema 1:** campos de receita e or√ßamento vazios na maior parte dos filmes. Isso ocorreu devido √† utiliza√ß√£o das classes da API do TMDB, ao inv√©s de fazer requisi√ß√µes com a URL e com a biblioteca `requests`.
- **Solu√ß√£o:** realizar novamente a ingest√£o da API e fazer o upload na camada Raw.
    - O script desenvolvido est√° neste diret√≥rio ([scriptnovoingestaoAPI.py](nova-ingestao/scriptnovoingestaoAPI.py))
    - Optei por fazer tudo localmente para n√£o comprometer tanto tempo quanto necessitou na Sprint 7.
    - Selecionei apenas filmes entre 2000 e 2010, limitando por 100 a cada ano, al√©m de buscar todos os filmes de Christopher Nolan.
        - Limitei a quantidade devido √† imensid√£o de filmes, tornando o script extremamente demorado para ser executado.

![Imagem execu√ß√£o do script novo de ingest√£o](../evidencias/0.1-execucao-script-novo.png)

![Imagem execu√ß√£o script novo filmes de Nolan](../evidencias/0.2-execucao-script-novo-nolan.png)

> Pode-se perceber que fiz um script para buscar os filmes entre 2000 e 2010, e outro para buscar os filmes de Christopher Nolan

**Problema 2:** eu n√£o havia buscado os nomes dos diretores dos filmes, causando um problema na hora de realizar a modelagem dimensional dos dados. Por isso, resolvi buscar os diretores de cada filme tamb√©m.

**Problema 3:** julguei n√£o serem mais necess√°rias as produtoras para a minha an√°lise.


**2. UPLOAD S3**
- Ap√≥s a execu√ß√£o dos scripts e a cria√ß√£o dos arquivos, realizei o upload no S3 conforme a figura abaixo:

![Imagem upload arquivos novos JSON no S3](../evidencias/0.3-uploadS3.png)


**3. PROCESSAMENTO DA CAMADA TRUSTED**

1. Alterei o par√¢metro dos arquivos de entrada, passando o novo caminho dos dados (com a data de hoje: 17/10).
2. Alterei o script para que recebesse todos os dados e os processasse corretamente, haja vista que alguns deles sofreram modifica√ß√µes.
3. Executei o job e obtive o resultado abaixo

![Imagem execu√ß√£o job](../evidencias/0.4-processamentoGlue.png)

**4. CRIA√á√ÉO DO CRAWLER PARA OS NOVOS DADOS**

- Ap√≥s, criei um novo crawler para os dados novos.

![Imagem cria√ß√£o crawler](../evidencias/0.5-criacaoCrawler.png)

**5. CONSULTA NO ATHENA**

- Por fim, consultei no Amazon Athena para ver se os dados estavam corretos.

![Imagem consulta no Athena](../evidencias/0.6-consultaAthena.png)



### 1. Quest√µes para an√°lise

As quest√µes para an√°lise foram alteradas, seguem quase as mesmas definidas na Sprint 7:

1. Quais foram os principais marcos que consolidaram Christopher Nolan como um dos diretores mais influentes do cinema mundial? 

2. Bilheteria de todos os filmes do ano 2000 e comparar com Memento (Amn√©sia) - primeiro filme de sucesso de Nolan.

3. Como foi o desempenho dos filmes do Batman em compara√ß√£o aos outros filmes do diretor (nota m√©dia e bilheteria)?

4. Para colocar suas ideias em pr√°tica, Nolan precisa de or√ßamentos maiores do que outros filmes similares?


### 2. Modelagem dos dados

Durante a realiza√ß√£o da modelagem dos dados, revisitei conceitos abordados na **Sprint 2** sobre modelagem dimensional, especificamente *star schema* e *snowflake*. Utilizei a ferramenta **BRModelo** para criar o diagrama de entidade-relacionamento e definir as tabelas de fatos e dimens√µes. Abaixo, apresento a estrutura do modelo e as respectivas explica√ß√µes:

#### Estrutura do Modelo

![Imagem modelagem dos dados](../evidencias/2-modelagem.png)

1. **Dimens√£o Tempo (`dim_tempo`)**:
   - Esta tabela cont√©m a **data de lan√ßamento** do filme, al√©m dos campos **ano**, **m√™s** e **dia**, facilitando an√°lises temporais. A inclus√£o desses campos permite consultas e relat√≥rios baseados em per√≠odos espec√≠ficos, como por ano ou por m√™s.

2. **Dimens√£o Diretor (`dim_diretor`)**:
   - Armazena os **nomes dos diretores** do filme. 

3. **Dimens√£o G√™nero (`dim_genero`)**:
   - Cont√©m o(s) **g√™nero(s)** dos filmes. Para lidar com filmes que possuem m√∫ltiplos g√™neros, foi utilizada uma **tabela de ponte** (`filme_genero`), que resolve o relacionamento muitos-para-muitos entre filmes e g√™neros.

4. **Dimens√£o T√≠tulo (`dim_titulo`)**:
   - Esta dimens√£o cont√©m os **t√≠tulos** dos filmes. Essa informa√ß√£o textual √© crucial para identifica√ß√£o e consultas por nome de filme.

5. **Fato Filme (`fato_filme`)**:
   - A tabela de fatos armazena as m√©tricas num√©ricas e dados quantitativos do conjunto de dados. As colunas principais s√£o:
     - **id**: Chave prim√°ria do filme.
     - **numero_votos**: Total de votos que o filme recebeu.
     - **nota_media**: A m√©dia das notas atribu√≠das ao filme.
     - **orcamento**: O or√ßamento destinado √† produ√ß√£o do filme.
     - **receita**: A receita gerada pelo filme.

#### Ajustes de Cardinalidade

Durante a modelagem, algumas cardinalidades foram ajustadas devido a limita√ß√µes da ferramenta **BRModelo**, onde todas as rela√ß√µes foram representadas como **1-n**. No entanto, as corre√ß√µes conceituais s√£o as seguintes:
  - Entre **fato_filme** e **dim_tempo**, a cardinalidade correta deveria ser **1-1**, pois um filme possui uma √∫nica data de lan√ßamento.
  - Entre **fato_filme** e **dim_genero**, a cardinalidade deveria ser **1-n**, e n√£o **0-n**, j√° que cada filme possui ao menos um g√™nero.

O restante da modelagem est√° correto, com a rela√ß√£o de muitos-para-muitos entre **fato_filme** e **dim_genero** resolvida pela tabela de ponte **filme_genero**.

### 3. Cria√ß√£o do script local

Primeiramente optei por criar o script *on-premise* para posteriormente levar ao AWS Glue e executar em um job.

**3.1 - Download dos arquivos .parquet**

Busquei os arquivos .PARQUET processados na Sprint anterior e localizados na camada Trusted Zone para t√™-los em minha m√°quina.

![Imagem download parquet](../evidencias/3.1-download-parquet.png)

**3.2 - Cria√ß√£o do script**

- **3.2.1**
    - Primeiro, criei um script que lia os arquivos .parquet e exibia os schemas, para entender todos os campos e como faria organizaria os dados para a modelagem efetuada.

![Imagem printSchemas](../evidencias/3.2-schemas.png)

- **3.2.2**
    - Posteriormente, realizei alguns testes de jun√ß√£o entre os dados do CSV (batch) e do TMDB, unindo pelo id do imdb, dado que busquei nessa mudan√ßa que realizei nesta Sprint.
    - Al√©m disso, removi campos desnecess√°rios, que n√£o seriam efetivamente utilizados na an√°lise final.

![Imagem dataframes unidos e limpos](../evidencias/3.3-dfLimpo.png) 

- [SCRIPT LOCAL](script-local.py)


### 4. Job AWS Glue

**4.1 - Cria√ß√£o do job**

Primeiramente, criei o job no AWS Glue conforme solicitado no enunciado do exerc√≠cio. A imagem abaixo comprova a cria√ß√£o do job.

![Imagem cria√ß√£o do job no AWS Glue](../evidencias/4.1-criacao-job.png)

**4.2 - Ajuste de par√¢metros do job**

Agora, inseri os par√¢metros com local de origem dos dados (camada Trusted) e onde eles ser√£o salvos ap√≥s execu√ß√£o do script (camada Refined).

![Imagem par√¢metros job](../evidencias/4.2-parametros-job.png)

**4.3 - Cria√ß√£o do script do job**

Com o job criado e os par√¢metros definidos, iniciei a constru√ß√£o do [script](script-glue.py) do job no Glue. Para isso, utilizei como base o script local desenvolvido anteriormente.

1. Imports necess√°rios

![Imagem imports](../evidencias/4.3-script1.png)

- O c√≥digo importa bibliotecas essenciais para executar um job no AWS Glue e manipular dados com PySpark.
- Bibliotecas essenciais para o uso do AWS Glue, o qual √© utilizado para ETL (*Extract*, *Transform*, *Load*) na AWS.
- PySpark: usado para manipula√ß√£o e processamento de dados distribu√≠dos.
- Fun√ß√£o `col` do PySpark para tratamento das colunas.


2. Configura√ß√µes de ambiente (par√¢metros, SparkContext e caminhos de arquivos)

![Imagem configura√ß√µes de ambiente](../evidencias/4.3-script2.png)

- **Linha 11:** `getResolvedOptions` obt√©m os par√¢metros necess√°rios para rodar o job. Eles incluem o nome do job, o caminho de entrada `(S3_INPUT_PATH)`, e o caminho de sa√≠da `(S3_TARGET_PATH)`.
- **Linhas 14-18:** configura√ß√µes necess√°rias para script no AWS Glue (fornecido pelo pr√≥prio servi√ßo ao criar o job).
- **Linhas 20-21:** defini√ß√£o dos caminhos dos arquivos de entrada e sa√≠da, Trusted e Refined Zone respectivamente.
- **Linhas 23-24:** Leitura dos arquivos PARQUET para Spark DataFrames: `df_csv` com dados da ingest√£o batch (local) e `df_tmdb` com os dados da ingest√£o da API do TMDB.


3. Transforma√ß√µes nos DataFrames

![Imagem transforma√ß√µes DataFrames](../evidencias/4.3-script3.png)

- **Linha 26:** removi o id do `df_tmdb`, pois utilizarei apenas o id do IMDB.
- **Linha 29:** jun√ß√£o dos DataFrames pelo id do TMDB.
- **Linha 32:** removi alguns campos que julguei n√£o serem mais necess√°rios para a minha an√°lise final e tamb√©m removi os duplicados pelo id do filme.
- **Linhas 35-38:** criei as colunas `ano`, `mes`, `dia` e `id_fato_filme` que seriam posteriormente necess√°rias.
   - Para criar o campo `id_fato_filme`, utilizei o m√©todo `monotonically_increasing_id()` que gera um long para cada registro, uma esp√©cie de contador.
- **Linha 41:** printei o Schema do DataFrame para entender como havia ficado ap√≥s essas transforma√ß√µes.


4. Ajuste no local de salvamento dos arquivos PARQUET

![Imagem salvamento PARQUET Movies](../evidencias/4.3-script4.png)

- Necessitei ajustar o local de salvamento dos arquivos PARQUET, inserindo mais um diret√≥rio: "Movies".

5. Tabela Dimensional `dim_titulo`

![Imagem tabela dim_titulo](../evidencias/4.3-script5.png)

- A tabela armazena o id e o t√≠tulo do filme, e os dados s√£o salvos no diret√≥rio /dim_titulo.

6. Tabela Dimensional `dim_tempo`

![Imagem tabela dim_tempo](../evidencias/4.3-script6.png)

- Colunas ano, mes, e dia que foram criadas anteriormente s√£o extra√≠das da coluna data_lancamento, al√©m da cria√ß√£o de um campo id_tempo para ser chave estrangeira na tabela de fatos.
- Os dados s√£o gravados no diret√≥rio /dim_tempo.

7. Tabela Dimensional `dim_diretor`

![Imagem tabela dim_diretor](../evidencias/4.3-script7.png)

- Cria um id para cada diretor, al√©m de selecionar a coluna "diretor" do DataFrame, que cont√©m o nome de cada um deles. Salva no diret√≥rio /dim_diretor.


8. Tabela Dimensional `dim_genero`

![Imagem tabela dim_genero](../evidencias/4.3-script8.png)

- Seleciona o campo `genero` do DataFrame principal. No entanto, esse √© uma string em que pode haver mais de um g√™nero, sendo necess√°rio um tratamento nesse sentido. 
   - Para isso utilizei os m√©todos `explode()` e `split()` para separar os g√™neros.
   - Ap√≥s, gerei um id para cada, utilizando o m√©todo `distinct()`, que pega apenas os g√™neros diferentes para criar ids.

9. Tabela Fato `fato_filme`

![Imagem tabela fato_filme](../evidencias/4.3-script9.png)

- Essa tabela possui todos os campos num√©ricos (`notaMedia`, `popularidade`, `numeroVotos`, `orcamento`, `receita`).
- Possui tamb√©m os ids das dimens√µes (chaves estrangeiras): `id` (id_imdb), `id_tempo` e `id_diretor`.

10. Tabela de ponte `filme_genero`

Como j√° foi explicitado acima, essa tabela serve para relacionar filmes e g√™neros, haja vista que √© uma rela√ß√£o N para N.

![Imagem tabela de ponte filme_genero](../evidencias/4.3-script10.png)

- Ela faz um join entre a tabela `fato_filme` e a tabela `dim_genero`, relacionando os filmes ao(s) seu(s) g√™nero(s).

 
11. Finaliza√ß√£o do job

![Imagem job commit](../evidencias/4.3-script91.png)


**4.4 - Job Runs**

Ap√≥s desenvolver o script, executei o job. Fiz isso por diversas vezes, pois obtive erros no meio do caminho, os quais tiveram de ser solucionados resultando no script explicitado no item anterior.

![Imagem job runs](../evidencias/4.4-jobRuns.png)

**4.5 - Diret√≥rios criados**

Os diret√≥rios criados ap√≥s a execu√ß√£o do job ficaram da seguinte maneira:

![Imagem diret√≥rios p√≥s execu√ß√£o do job](../evidencias/4.5-diretoriosExecucao.png)


### 5. Crawler

**5.1 - Cria√ß√£o**

Para criar as tabelas conforme requisitado, necessitei criar novamente um *crawler*, da mesma forma que na Sprint anterior. A imagem a seguir comprova a cria√ß√£o do Crawler, com a origem dos dados sendo a Refined Zone.

![Imagem cria√ß√£o Crawler](../evidencias/5.1-criandoCrawler.png)

**5.2 - Execu√ß√£o**

Ap√≥s criar o *crawler*, executei-o para que ele criasse as tabelas baseado nos arquivos PARQUET gerados.

![Imagem execu√ß√£o crawler](../evidencias/5.2runCrawler.png)

**5.3 - Consulta das tabelas criadas**

Com isso, verifiquei se as tabelas haviam sido criadas corretamente no AWS Glue Data Catalog.

![Imagem tabelas criadas](../evidencias/5.3-tabelasCriadas.png)

**5.4 - Consulta no Amazon Athena**

Por fim, performei uma consulta no Amazon Athena para ver se as tabelas estavam preenchidas corretamente conforme o script desenvolvido.

![Imagem consulta Athena](../evidencias/5.4-consultaAthena.png)

___

### ‚Ü©Ô∏è [Retornar ao in√≠cio](../../README.md)