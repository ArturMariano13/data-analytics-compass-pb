# üß© Desafio da Sprint 9
Este diret√≥rio cont√©m os arquivos necess√°rios para a realiza√ß√£o do desafio desta Sprint.

___

## üìù Enunciado
O desafio da Sprint 9 √© uma continua√ß√£o do desafio iniciado na Sprint 6, sendo a quarta entrega do desafio final.

Esta etapa consiste na **modelagem dos dados e processamento da Camada *refined***. Nessa camada, os dados devem estar prontos para an√°lise e extra√ß√£o de insights. A origem correspondente desses dados deve ser a camada Trusted, processada na Sprint 8.

Nesta Sprint devemos pensar em estruturar os dados seguindo os princ√≠pios de modelagem multidimensional, a fim de permitir consultas sobre diferentes perspectivas.

Ser√° novamente utilizado o Apache Spark no processo, utilizando jobs cuja origem sejam dados da Trusted Zone, e o destino a camada Refined Zone. Os dados devem ser novamente persistidos no formato PARQUET, particionados, se necess√°rio, de acordo com as necessidades definidas para a camada de visualiza√ß√£o.

### Afazeres
- Criar tabelas no AWS Glue Data Catalog.
- Se necess√°rio, criar *views* de acordo com a modelagem de dados solicitada.
- Criar camada *Refined*, tendo como origem os dados da camada *Trusted*.

### Entreg√°veis
- Arquivo markdown (este README) com evid√™ncias da realiza√ß√£o do desafio + documenta√ß√£o de cada parte executada.
- Modelo de dados da camada Refined desenhado em ferramenta de modelagem.
- C√≥digo desenvolvido com devidos coment√°rios.

--- 

## Resolu√ß√£o

### 1. Quest√µes para an√°lise

As quest√µes para an√°lise n√£o foram alteradas, seguem as mesmas definidas na Sprint 7:

1. Quais foram os principais marcos que consolidaram Christopher Nolan como um dos diretores mais influentes do cinema mundial? 

2. M√©dia de bilheteria de todos os filmes do ano 2000 e comparar com Memento (Amn√©sia) - primeiro filme de sucesso de Nolan.

3. Avalia√ß√£o m√©dia de filmes de her√≥is em compara√ß√£o √† m√©dia dos filmes do Batman de Nolan.

4. Para colocar suas ideias em pr√°tica, Nolan precisa de or√ßamentos maiores do que outros filmes similares?


### 2. Modelagem dos dados

Por conseguinte, realizei a modelagem dos dados. Necessitei rever conceitos aprendidos e aplicados na Sprint 2 (modelagem dimensional - *star schema* e *snowflake*) para realizar essa tarefa.

Para realizar a modelagem, utilizei a ferramenta **brmodelo**, criando os seguintes fatos e dimens√µes:

![Imagem modelagem dos dados](../evidencias/2-modelagem.png)

- **dim_tempo:** cont√©m a data de lan√ßamento do filme, al√©m dos campos ano, m√™s e dia, para facilitar an√°lises posteriores.
- **dim_diretor:** cont√©m o nome do diretor, no caso apenas filmes de Christopher Nolan conter√£o, os outros n√£o foram coletados, pois n√£o havia necessidade.
- **dim_genero:** cont√©m o(s) g√™nero(s) dos filmes.
- **dim_titulo:** cont√©m o t√≠tulo dos filmes.
- **fato_filme:** considerei o filme como o fato, contendo todas as informa√ß√µes num√©ricas do meu dataset: id, n√∫mero de votos, nota m√©dia, or√ßamento e receita.

As cardinalidades s√£o todas **1-n** devido √† limita√ß√£o da ferramenta brmodelo, por√©m entre o fato e a dimens√£o tempo deveria ser **1-1**, e a cardinalidade do lado da dimens√£o g√™nero deveria ser **1-n**, n√£o 0-n. O restante est√° correto.


### 3. Cria√ß√£o do script local

Primeiramente optei por criar o script *on-premise* para posteriormente levar ao AWS Glue e executar em um job.

**3.1 - Download dos arquivos .parquet**

Busquei os arquivos .PARQUET processados na Sprint anterior e localizados na camada Trusted Zone para t√™-los em minha m√°quina.

![Imagem download parquet](../evidencias/3.1-download-parquet.png)

**3.2 - Cria√ß√£o do script**

- **3.2.1**
    - Primeiro, criei um script que lia os arquivos .parquet e exibia os schemas, para entender todos os campos e como faria organizaria os dados para a modelagem efetuada.

![Imagem printSchemas](../evidencias/3.2-schemas.png)

- **3.2.2**
    - Posteriormente, realizei alguns testes de jun√ß√£o entre os dados do CSV (batch) e do TMDB, unindo pelo t√≠tulo do filme, haja vista que os ids eram diferentes.
    - Al√©m disso, removi campos desnecess√°rios, que n√£o seriam efetivamente utilizados na an√°lise final.

![Imagem dataframes unidos e limpos](../evidencias/3.3-dfLimpo.png) 








### 4. Job AWS Glue

**4.1 - Cria√ß√£o do job**

Primeiramente, criei o job no AWS Glue conforme solicitado no enunciado do exerc√≠cio. A imagem abaixo comprova a cria√ß√£o do job.

![Imagem cria√ß√£o do job no AWS Glue](../evidencias/4.1-criacao-job.png)

**4.2 - Ajuste de par√¢metros do job**

Agora, inseri os par√¢metros com local de origem dos dados (camada Trusted) e onde eles ser√£o salvos ap√≥s execu√ß√£o do script (camada Refined).

![Imagem par√¢metros job](../evidencias/4.2-parametros-job.png)

**4.3 - Cria√ß√£o do script do job**






___

### ‚Ü©Ô∏è [Retornar ao in√≠cio](../../README.md)