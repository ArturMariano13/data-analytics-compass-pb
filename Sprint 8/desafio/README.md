# üß© Desafio da Sprint 8
Este diret√≥rio cont√©m os arquivos necess√°rios para a realiza√ß√£o do desafio desta Sprint.

___

## üìù Enunciado
O desafio da Sprint 8 √© uma continua√ß√£o do desafio iniciado na Sprint 6, sendo a terceira entrega do desafio final.

Esta etapa consiste no **processamento da camada *trusted***, com essa possuindo os dados limpos e confi√°veis. Consiste na integra√ß√£o das diversas fontes de origem (dados que est√£o na camada Raw).

Ser√° utilizado Apache Spark atrav√©s do servi√ßo AWS Glue, integrando dados existentes na camada *Raw Zone* para a *Trusted Zone*. Todos os dados da *Trusted Zone* devem possuir o mesmo formato de armazenamento e poder ser analisados no AWS Athena por meio de SQL.

Os dados ser√£o persistidos no formato PARQUET, particionados por data de cria√ß√£o do arquivo no momento da ingest√£o do dado da TMDB. A exce√ß√£o fica para os dados oriundos do processamento *batch* (CSV), que n√£o precisam ser particionados.

Iremos separar o processamento em dois jobs:
1. Processamento dos arquivos CSV
2. Processamento dos dados oriundos da API TMDB.

> OBS.: N√£o utilizar notebooks do Glue.


## Resolu√ß√£o

### 1. Cria√ß√£o e permiss√µes de usu√°rio IAM

Na Sprint anterior, no exerc√≠cio Lab AWS Glue, tivemos de criar um usu√°rio IAM para fornecer permiss√µes e realizar as opera√ß√µes necess√°rias. No entanto, ao encerrar a Sprint, exclu√≠ todos os recursos criados por mim durante a Sprint, necessitando agora criar outro usu√°rio IAM.

Sendo assim, criei o usu√°rio e configurei a *Role* para o Glue.

![Imagem cria√ß√£o IAM user](../evidencias/ev_desafio/1-usuarioIAM.png)

![Imagem configura√ß√£o IAM Role](../evidencias/ev_desafio/1.1-configRole1.png)

![Imagem configura√ß√£o IAM Role 2](../evidencias/ev_desafio/1.2-configRole2.png)

### 2. Configura√ß√µes de permiss√µes no AWS Lake Formation

**2.1. Cria√ß√£o de *Database***

![Imagem cria√ß√£o database](../evidencias/ev_desafio/2-criacaoDatabase.png)

**2.2. Adi√ß√£o do usu√°rio IAM como Administrador do *data lake***

![Imagem configura√ß√£o Administrador data lake](../evidencias/ev_desafio/2.1-confLakeFormation.png)

**2.3. Altera√ß√£o de permiss√µes do *Database***

![Imagem altera√ß√£o de permiss√µes do database](../evidencias/ev_desafio/2.2-grantPermissions.png)

### 3. Cria√ß√£o dos jobs

Cada um dos jobs fica respons√°vel por dados ingeridos de maneira distinta:

- **job-batch-data**: dados CSV processados em lote (Sprint 6).

![Imagem cria√ß√£o job dados ingest√£o Batch](../evidencias/ev_desafio/3-criacaoJobBatch.png)

- **job-tmdb-data**: dados JSON ingeridos da API do TMDB (Sprint 7).

![Imagem cria√ß√£o job dados API TMDB](../evidencias/ev_desafio/3.1-criacaoJobTMDB.png)


![Imagem jobs AWS Glue](../evidencias/ev_desafio/3.2-jobsCriados.png)

Por conseguinte, foi necess√°rio alterar os par√¢metros com os locais de busca dos arquivos Raw e de salvamento dos arquivos na camada Trusted.

![Imagem altera√ß√£o de par√¢metros para Script](../evidencias/ev_desafio/3.3-alteracaoParametros.png)

### 4. Desenvolvimento dos Scripts

### 4.1. Script dados *Batch*

1. **Importa√ß√µes e depend√™ncias**

![Imagem bibliotecas script](../evidencias/ev_desafio/4.1.1-bibliotecas.png)

- `sys`: para acessar argumentos de linha de comando.
- `awsglue.transforms, awsglue.utils, awsglue.context, awsglue.job`: bibliotecas da AWS Glue para trabalhar com jobs, criar contextos e transformar dados.
- `pyspark.sql.functions` e `pyspark.sql.types`: fun√ß√µes e tipos de dados do PySpark para manipula√ß√£o e convers√£o de colunas no DataFrame.
- `datetime`: utilizado para gerar o timestamp da execu√ß√£o.


2. **Defini√ß√£o dos par√¢metros do job**

![Imagem par√¢metros do job](../evidencias/ev_desafio/4.1.2-parametros.png)

Aqui s√£o obtidos os argumentos passados ao job (nome do job, caminho de entrada e sa√≠da no S3).


3. **Cria√ß√£o do contexto do Spark e inicializa√ß√£o do job**

![Imagem cria√ß√£o de contexto do Spark](../evidencias/ev_desafio/4.1.3-contextoSpark.png)

- `SparkContext`: inicializa o contexto do Spark.
- `GlueContext`: cria um contexto do Glue para utilizar suas funcionalidades no Spark.
- `spark_session`: cria uma sess√£o Spark para manipular DataFrames.
- `job.init`: inicializa o job Glue usando o nome passado via par√¢metro.


4. **Defini√ß√£o de vari√°veis e prepara√ß√£o de caminhos**

![Imagem defini√ß√£o de vari√°veis e prepara√ß√£o de caminhos](../evidencias/ev_desafio/4.1.4-variaveisCaminhos.png)

- `now`: armazena a data e hora atuais.
- `DATA_ATUAL`: formata a data para ser utilizada nos caminhos de sa√≠da.
- `source_file`: caminho do arquivo de entrada na "Raw Zone".
- `target_path`: caminho do arquivo de sa√≠da na "Trusted Zone", que inclui a data no caminho.


5. **Leitura dos dados da Raw Zone**

![Imagem leitura de dados da Raw Zone](../evidencias/ev_desafio/4.1.5-leituraDadosRawZone.png)

- `create_dynamic_frame`: l√™ o arquivo CSV diretamente do S3.
- `withHeader`: indica que o arquivo CSV cont√©m um cabe√ßalho.
- `separator`: define o separador de colunas (neste caso, √© **|**).


6. **Convers√£o para DataFrame Spark**

Converti o *DynamicFrame* (estrutura nativa do AWS Glue) para um DataFrame do Spark para facilitar as transforma√ß√µes.

![Imagem convers√£o para DataFrame Spark](../evidencias/ev_desafio/4.1.6-conversaoDataFrameSpark.png)


7. **Convers√£o de colunas para os tipos apropriados**

![Imagem convers√£o de colunas](../evidencias/ev_desafio/4.1.7-conversaoTiposDados.png)

- anoLancamento: para *IntegerType*.
- notaMedia: para *FloatType*.
- numeroVotos: para *IntegerType*.


8. **Sele√ß√£o das colunas desejadas**

Selecionei apenas as colunas relevantes do DataFrame original: ID, t√≠tulos (principal e original), ano de lan√ßamento, g√™nero, nota m√©dia e n√∫mero de votos.

![Imagem sele√ß√£o de colunas desejadas](../evidencias/ev_desafio/4.1.8-selecaoDados.png)


9. **Filtragem de filmes**

- Filtragem dos filmes que foram lan√ßados entre os anos de 2000 e 2020.

![Imagem filmes entre 2000 e 2020](../evidencias/ev_desafio/4.1.9-filmesSeculo.png)

- Filtragem dos filmes em que Heath Ledger atuou.

![Imagem filmes de Heath Ledger](../evidencias/ev_desafio/4.1.10-heathLedger.png)


10. **Remo√ß√£o de duplicados pelo id**

![Imagem remo√ß√£o de duplicados por id](../evidencias/ev_desafio/4.1.11-remocaoDuplicados.png)


11. **Salvamento do resultado no S3 em formato Parquet**

![Imagem salvamento em Parquet no S3](../evidencias/ev_desafio/4.1.12-salvamentoParquet.png)


12. **Finaliza√ß√£o do job**

![Imagem finaliza√ß√£o do job](../evidencias/ev_desafio/4.1.13-finalizacaoJob.png)


13. **Execu√ß√£o do Script**

- Job runs:

![Imagem execu√ß√£o script dados Batch - job runs](../evidencias/ev_desafio/4.1.14-execucaoScript.png)

- Estrutura de diret√≥rios:

![Imagem execu√ß√£o script dados Batch - diret√≥rios](../evidencias/ev_desafio/4.1.15-execucaoDiretorios.png)


### 4.2. Script dados API TMDB

1. **Bibliotecas necess√°rias**

![Imagem importa√ß√µes de bibliotecas](../evidencias/ev_desafio/4.2.1-bibliotecas.png)


2. **Par√¢metros**

![Imagem par√¢metros do job](../evidencias/ev_desafio/4.2.2-parametros.png)


3. **Inicializa√ß√£o do contexto Glue**

![Imagem contexto Glue](../evidencias/ev_desafio/4.2.3-contextoGlue.png)


4. **Defini√ß√£o de Caminhos**

![Imagem defini√ß√£o dos caminhos](../evidencias/ev_desafio/4.2.4-caminhos.png)


5. **Leitura de arquivos JSON**

Para realizar a leitura dos arquivos JSON utilizei o DataFrame do Spark diretamente, sem utilizar o DynamicFrame (nativo do Glue). Isso, pois inicialmente tentei utilizar a mesma estrat√©gia do passo anterior, para ler o arquivo CSV, mas n√£o obtive sucesso. Modificando isso, consegui realizar corretamente.

![Imagem leitura dos arquivos JSON](../evidencias/ev_desafio/4.2.5-leituraArquivosJSON.png)


6. **Convers√£o de Colunas**

![Imagem convers√£o de coluna release_date](../evidencias/ev_desafio/4.2.6-conversaoReleaseDate.png)

Converte a coluna *release_date* para o formato de data, renomeando-a para *data_lancamento*.


7. **Captura do Caminho do Arquivo S3**

![Imagem adi√ß√£o de nome do arquivo do S3](../evidencias/ev_desafio/4.2.7-capturaNomeArquivo.png)

Adiciona uma nova coluna *file_path* que cont√©m o caminho do arquivo S3 de onde os dados foram lidos.


8. **Extra√ß√£o de Data a Partir do Caminho do Arquivo**

![Imagem extra√ß√£o ano, m√™s e dia](../evidencias/ev_desafio/4.2.8-extracaoAnoMesDia.png)

Utiliza express√µes regulares para extrair o ano, m√™s e dia do caminho do arquivo, armazenando-os em novas colunas ano, mes e dia.


9. **Cria√ß√£o da Coluna de Data de Cria√ß√£o**

![Imagem cria√ß√£o de coluna data de cria√ß√£o](../evidencias/ev_desafio/4.2.9-criacaoColunaDataCriacao.png)

Combina as colunas de ano, m√™s e dia em uma nova coluna chamada `data_criacao`, convertendo-a para o tipo de dado `DateType`.


10. **Corre√ß√£o da Coluna de Empresas de Produ√ß√£o**

![Imagem corre√ß√£o coluna production_companies](../evidencias/ev_desafio/4.2.10-correcaoProductionCompanies.png)

Concatena os nomes das empresas de produ√ß√£o em uma √∫nica string, separando-os por v√≠rgulas.


11. **Sele√ß√£o das Colunas de Interesse**

![Imagem sele√ß√£o de campos desejados](../evidencias/ev_desafio/4.2.11-selecaoCampos.png)


12. **Salvamento dos Dados em Formato Parquet**

![Imagem salvamento dos dados em Parquet](../evidencias/ev_desafio/4.2.12-salvamentoDF.png)


13. **Finaliza√ß√£o do Job**

![Imagem finaliza√ß√£o do Job](../evidencias/ev_desafio/4.2.13-finalizacaoJob.png)


14. **Execu√ß√£o script**

- Job runs:

![Imagem execu√ß√£o script dados TMDB - job runs](../evidencias/ev_desafio/4.2.14-execucaoScript.png)

**Execu√ß√£o do Script**
- Estrutura de diret√≥rios: 

![Imagem execu√ß√£o script dados TMDB - diret√≥rios](../evidencias/ev_desafio/4.2.15-execucaoDiretorios.png)


### 5. Cria√ß√£o de Crawler

Depois, necessitamos criar um Crawler para criar uma tabela a partir dos dados do S3 automaticamente.

![Imagem cria√ß√£o Crawlers](../evidencias/ev_desafio/5-crawlersCriados.png)

Sendo assim, os crawlers criados geraram as tabelas abaixo:

![Imagem tabelas criadas](../evidencias/ev_desafio/5.1-tabelasCriadas.png)

### 6. Consultando dados com Athena

**6.1. Consulta na tabela dos dados CSV**

![Imagem select tudo](../evidencias/ev_desafio/6.1-selectBatch.png)

![Imagem resultados select](../evidencias/ev_desafio/6.1.1-selectBatchResultado.png)


**6.2. Consulta na tabela dos dados JSON (TMDB)**

Primeiramente, ao tentar realizar *selects* na tabela "parquet", criada com os dados provenientes do TMDB, obtive uma tabela sem registros (vazia). Para isso, necessitei dar o seguinte comando: 

![Imagem comando corre√ß√£o select](../evidencias/ev_desafio/6.2.3-comandoParticao.png)

O comando `MSCK REPAIR TABLE nometabela` √© utilizado no contexto do Apache Hive e do Amazon Athena, que s√£o ferramentas de processamento de dados que trabalham com grandes volumes de informa√ß√µes armazenadas em sistemas distribu√≠dos, como o Amazon S3. A fun√ß√£o desse comando √© manter a integridade da tabela que est√° sendo utilizada para consultar dados particionados.

Nesse caso, como a tabela possui dados particionados, necessitei executar esse comando.

Ap√≥s isso realizado, pude realizar os comandos de sele√ß√£o na tabela:

![Imagem select tudo TMDB](../evidencias/ev_desafio/6.2-selectTMDB.png)

![Imagem resultados select](../evidencias/ev_desafio/6.2.1-selectTMDBResultados.png)

![Imagem resultados select 2](../evidencias/ev_desafio/6.2.2-selectTMDBResultados.png)

___

### ‚Ü©Ô∏è [Retornar ao in√≠cio](../../README.md)